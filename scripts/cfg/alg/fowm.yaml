_target_: forl.algorithms.fowm.FOWM
_recursive_: False
actor_config:
  _target_: forl.models.actor.ActorStochasticMLP
  units: ${resolve_child:[400, 200, 100],${env.shac.actor_mlp},units}
  activation_class: nn.Mish
  init_gain: 1.0
  init_logstd: -1.0
critic_config:
  _target_: forl.models.critic.CriticMLP
  units: ${resolve_child:[400, 200],${env.shac.critic_mlp},units}
  activation_class: nn.Mish
world_model_config:
  multitask: False
  num_enc_layers: 2
  enc_dim: 256
  num_channels: 32
  mlp_dim: 512
  latent_dim: 512
  task_dim: 0
  num_q: 5
  dropout: 0.01
  simnorm_dim: 8
  obs_shape: {"state": [37]}
  action_dim: 8
  num_bins: 101
  vmin: -10
  vmax: +10
  log_std_min: -10
  log_std_max: 2
  tau: 0.01
  obs: "state" # UNSURE
  lr: 3e-4
  enc_lr_scale: 0.3
  reward_coef: 0.1
  value_coef: 0.1
  consistency_coef: 20
# world_model_path: /storage/home/hcoda1/7/igeorgiev3/git/FoWM/wmlab/logs/dflex-ant/10/pretrain_baseline/models/200000.pt # discrete reward
world_model_path: /storage/home/hcoda1/7/igeorgiev3/git/FoWM/wmlab/logs/dflex-ant/15/pretrain_baseline/models/40000.pt # continuous reward
num_critics: 3
actor_lr: ${resolve_child:1e-2,${env.shac},actor_lr}
critic_lr: ${resolve_child:5e-3,${env.shac},critic_lr}
lr_schedule: linear
obs_rms: True
ret_rms: True
critic_iterations: 16
critic_batches: 4
critic_method: td-lambda # ('td-lambda', 'one-step')
lam: 0.95
gamma: 0.99
max_epochs: ${resolve_child:2000,${env.shac},max_epochs}
horizon: 32
actor_grad_norm: 1.0 # Can also be none
critic_grad_norm: 100.0 # Can also be none
save_interval: ${resolve_child:400,${env.shac},save_interval}
device: ${general.device}
